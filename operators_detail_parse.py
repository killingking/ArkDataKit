import asyncio
import json
import re
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
from config import BASE_URL, PLAYWRIGHT_CONFIG, JSON_OUTPUT_DIR  # è¡¥å……JSON_OUTPUT_DIRå¯¼å…¥
from utils import logger, clean_text, clean_desc, clean_filename, ensure_output_dir

class OperatorDetailParser:
    """å¹²å‘˜è¯¦æƒ…è§£æå™¨ï¼ˆæœ‰çŠ¶æ€ç±»å°è£…ï¼Œç»´æŠ¤page/soupï¼‰"""
    # ========== å…³é”®ä¿®æ”¹1ï¼šæ–°å¢å…¨å±€å¤ç”¨çš„æµè§ˆå™¨/ä¸Šä¸‹æ–‡ ==========
    _shared_playwright = None
    _shared_browser = None
    _shared_context = None
    _browser_initialized = False

    # ========== å…³é”®ä¿®æ”¹2ï¼šç±»æ–¹æ³•åˆå§‹åŒ–å…¨å±€æµè§ˆå™¨ï¼ˆåªåˆ›å»º1æ¬¡ï¼‰ ==========
    @classmethod
    async def init_shared_browser(cls):
        """åˆå§‹åŒ–å…¨å±€å¤ç”¨çš„æµè§ˆå™¨å®ä¾‹ï¼ˆæ‰¹é‡çˆ¬å–æ—¶åªåˆ›å»º1æ¬¡ï¼‰"""
        if cls._browser_initialized:
            return cls._shared_context

        try:
            cls._shared_playwright = await async_playwright().start()
            # ä¼˜åŒ–æµè§ˆå™¨å¯åŠ¨å‚æ•°ï¼šç¦ç”¨æ²™ç®±ã€é™åˆ¶å†…å­˜ã€ç»•è¿‡/dev/shm
            browser_args = PLAYWRIGHT_CONFIG["browser_args"] + [
                "--no-sandbox",          # è§£å†³å°å†…å­˜æœåŠ¡å™¨å´©æºƒ
                "--disable-gpu",         # ç¦ç”¨GPUåŠ é€Ÿ
                "--disable-dev-shm-usage",# ç»•è¿‡å…±äº«å†…å­˜é™åˆ¶
                "--disk-cache-dir=/tmp/playwright-cache",  # æŒ‡å®šç¼“å­˜ç›®å½•
                "--max-old-space-size=512",  # é™åˆ¶Chromeå†…å­˜ï¼ˆ512Mï¼‰
                "--memory-pressure-off"  # å…³é—­å†…å­˜å‹åŠ›æ£€æµ‹
            ]
            # å¯åŠ¨æµè§ˆå™¨ï¼ˆå¤ç”¨æ ¸å¿ƒï¼‰
            cls._shared_browser = await cls._shared_playwright.chromium.launch(
                headless=PLAYWRIGHT_CONFIG["headless"],
                args=browser_args,
                timeout=60000  # æµè§ˆå™¨å¯åŠ¨è¶…æ—¶å»¶é•¿åˆ°60ç§’
            )
            # åˆ›å»ºå¤ç”¨çš„ä¸Šä¸‹æ–‡
            cls._shared_context = await cls._shared_browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            cls._browser_initialized = True
            logger.info("âœ… å…¨å±€æµè§ˆå™¨å®ä¾‹åˆå§‹åŒ–å®Œæˆï¼ˆå¤ç”¨æ¨¡å¼ï¼‰")
            return cls._shared_context
        except Exception as e:
            logger.error(f"âŒ å…¨å±€æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
            await cls.close_shared_browser()
            raise

    # ========== å…³é”®ä¿®æ”¹3ï¼šç±»æ–¹æ³•å…³é—­å…¨å±€æµè§ˆå™¨ï¼ˆæ‰¹é‡ç»“æŸåè°ƒç”¨ï¼‰ ==========
    @classmethod
    async def close_shared_browser(cls):
        """å…³é—­å…¨å±€æµè§ˆå™¨å®ä¾‹ï¼ˆæ‰¹é‡çˆ¬å–ç»“æŸåæ‰§è¡Œï¼‰"""
        if cls._shared_context:
            await cls._shared_context.close()
        if cls._shared_browser:
            await cls._shared_browser.close()
        if cls._shared_playwright:
            await cls._shared_playwright.stop()
        cls._browser_initialized = False
        cls._shared_playwright = None
        cls._shared_browser = None
        cls._shared_context = None
        logger.info("ğŸ”Œ å…¨å±€æµè§ˆå™¨å®ä¾‹å·²å…³é—­")

    def __init__(self, operator_name: str):
        # åˆå§‹åŒ–é…ç½®å’ŒçŠ¶æ€
        self.operator_name = operator_name.strip()
        self.url = f"{BASE_URL}/w/{self.operator_name}" if self.operator_name else ""
        self.page = None  # Playwrighté¡µé¢å¯¹è±¡ï¼ˆçŠ¶æ€ï¼‰
        self.soup = None  # BeautifulSoupå¯¹è±¡ï¼ˆçŠ¶æ€ï¼‰
        
        # ä»ç»Ÿä¸€é…ç½®è¯»å–å‚æ•°
        self.term_min_length = PLAYWRIGHT_CONFIG["term_filter"]["min_length"]
        self.desc_min_length = PLAYWRIGHT_CONFIG["term_filter"]["desc_min_length"]
        self.tooltip_selectors = PLAYWRIGHT_CONFIG["tooltip_selectors"]
        self.wait_times = PLAYWRIGHT_CONFIG["wait_time"]
        self.timeouts = PLAYWRIGHT_CONFIG["timeout"]
        # åŸæœ‰browser_argsä¿ç•™ï¼ˆä½†å®é™…ç”¨å…¨å±€çš„ï¼‰
        self.browser_args = PLAYWRIGHT_CONFIG["browser_args"]
        self.headless = PLAYWRIGHT_CONFIG["headless"]

    # ========== å…³é”®ä¿®æ”¹4ï¼šé‡æ„_init_browser_pageï¼Œå¤ç”¨å…¨å±€æµè§ˆå™¨ ==========
    async def _init_browser_page(self):
        """å†…éƒ¨æ–¹æ³•ï¼šåˆå§‹åŒ–é¡µé¢ï¼ˆå¤ç”¨å…¨å±€æµè§ˆå™¨ï¼Œåªæ–°å»ºpageï¼‰"""
        if not self.operator_name:
            raise ValueError("âŒ å¹²å‘˜åç§°ä¸èƒ½ä¸ºç©º")
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # å¤ç”¨å…¨å±€æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œä¸å†æ–°å»ºbrowser
                context = await self.init_shared_browser()
                self.page = await context.new_page()
                
                # ä¼˜åŒ–è¶…æ—¶é…ç½®
                self.page.set_default_timeout(self.timeouts["page_load"] or 60000)  # è‡³å°‘60ç§’
                self.page.set_default_navigation_timeout(self.timeouts["page_load"] or 60000)
                
                # åŠ è½½é¡µé¢ï¼šæ”¹ä¸ºwait_until="load"ï¼ˆå®Œå…¨åŠ è½½ï¼‰+ å»¶é•¿è¶…æ—¶
                await self.page.goto(
                    self.url, 
                    wait_until="load",  # å…³é”®ï¼šä»domcontentloadedæ”¹ä¸ºload
                    timeout=60000       # é¡µé¢åŠ è½½è¶…æ—¶å»¶é•¿åˆ°60ç§’
                )
                # ç­‰å¾…æ ¸å¿ƒå†…å®¹+ç½‘ç»œç©ºé—²ï¼ˆè§£å†³åŠ¨æ€å†…å®¹åŠ è½½ä¸å…¨ï¼‰
                await self.page.wait_for_selector("#mw-content-text", timeout=60000)
                await self.page.wait_for_load_state("networkidle")  # ç­‰å¾…ç½‘ç»œç©ºé—²
                await asyncio.sleep(1)  # é¢å¤–ç­‰å¾…1ç§’
                logger.info(f"âœ… æµè§ˆå™¨é¡µé¢åˆå§‹åŒ–å®Œæˆï¼š{self.url}")
                return None  # ä¸å†è¿”å›browserï¼ˆå…¨å±€å¤ç”¨ï¼‰
                
            except Exception as e:
                if attempt == max_retries - 1:
                    raise Exception(f"âŒ é¡µé¢åˆå§‹åŒ–å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {str(e)}")
                
                logger.warning(f"âš ï¸ é¡µé¢åˆå§‹åŒ–å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt + 1}/{max_retries}): {str(e)}")
                # å¤±è´¥æ—¶å…³é—­å½“å‰pageï¼Œé¿å…æ³„æ¼
                if self.page:
                    await self.page.close()
                    self.page = None
                await asyncio.sleep(3)  # é‡è¯•é—´éš”å»¶é•¿åˆ°3ç§’

    async def _get_soup(self):
        """å†…éƒ¨æ–¹æ³•ï¼šå¤ç”¨soupå¯¹è±¡ï¼ˆé¿å…é‡å¤è§£æé¡µé¢ï¼‰"""
        if not self.soup and self.page:
            content = await self.page.content()
            self.soup = BeautifulSoup(content, "lxml")
        return self.soup

    async def parse_attrs(self):
        """è§£æå¹²å‘˜å±æ€§ï¼ˆåŸºç¡€å±æ€§+é¢å¤–å±æ€§ï¼‰â€”â€” ç²¾å‡†é€‚é…hidden_factionç»“æ„"""
        await self._get_soup()
        # åˆå§‹åŒ–åŸºç¡€å±æ€§ç»“æ„ï¼ˆåŸæœ‰é€»è¾‘ä¿ç•™ï¼Œæ— éœ€ä¿®æ”¹ï¼‰
        base_attrs = {
            "elite_0_level_1": {},
            "elite_0_max": {},
            "elite_1_max": {},
            "elite_2_max": {},
            "trust_bonus": {}
        }
        base_tbl = self.soup.select_one("table.char-base-attr-table")
        
        if base_tbl:
            headers = [clean_text(th) for th in base_tbl.select("tr:first-child th, tr:first-child td")]
            key_mapping = [
                "elite_0_level_1" if "ç²¾è‹±0 1çº§" in h else
                "elite_0_max" if "ç²¾è‹±0 æ»¡çº§" in h else
                "elite_1_max" if "ç²¾è‹±1 æ»¡çº§" in h else
                "elite_2_max" if "ç²¾è‹±2 æ»¡çº§" in h else
                "trust_bonus" if "ä¿¡èµ–åŠ æˆä¸Šé™" in h else
                "" for h in headers
            ]
            attr_mapping = {"ç”Ÿå‘½ä¸Šé™": "max_hp", "æ”»å‡»": "atk", "é˜²å¾¡": "def", "æ³•æœ¯æŠ—æ€§": "res"}
            
            for tr in base_tbl.select("tr")[1:]:
                tds = [clean_text(td) for td in tr.select("th, td")]
                if len(tds) < 2:
                    continue
                attr_key = attr_mapping.get(tds[0], tds[0].lower())
                for idx, val in enumerate(tds[1:], 1):
                    if idx < len(key_mapping) and key_mapping[idx]:
                        base_attrs[key_mapping[idx]][attr_key] = val

        # ========== é‡ç‚¹ä¿®å¤ï¼šé¢å¤–å±æ€§è§£æï¼ˆé€‚é…hidden_factionç»“æ„ï¼‰ ==========
        extra_attrs = {}
        # 1. å…¼å®¹å¤šç§é¢å¤–å±æ€§è¡¨æ ¼é€‰æ‹©å™¨
        extra_tbl = self.soup.select_one("table.char-extra-attr-table") or self.soup.select_one("table.wikitable.char-extra-attr")
        if not extra_tbl:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°é¢å¤–å±æ€§è¡¨æ ¼ï¼Œè·³è¿‡é¢å¤–å±æ€§è§£æ")
            return {"base_attributes": base_attrs, "extra_attributes": extra_attrs}

        # 2. è‡ªå®šä¹‰å·¥å…·å‡½æ•°ï¼šæå–æ ‡ç­¾å†…çš„çº¯æ–‡æœ¬ï¼ˆå¿½ç•¥åµŒå¥—span/é“¾æ¥ï¼‰
        def get_pure_text(elem) -> str:
            """æå–å…ƒç´ å†…çš„æ‰€æœ‰å¯è§æ–‡æœ¬ï¼ˆåˆå¹¶aæ ‡ç­¾/è¿‡æ»¤spanå›¾æ ‡ï¼‰"""
            if not elem:
                return ""
            # å…ˆç§»é™¤å›¾æ ‡ç±»spanï¼ˆé¿å…å¹²æ‰°æ–‡æœ¬ï¼‰
            for span in elem.find_all("span", class_=["mc-tooltips", "mdi"]):
                span.extract()
            # æå–æ‰€æœ‰æ–‡æœ¬ï¼ˆåŒ…æ‹¬aæ ‡ç­¾å†…çš„æ–‡æœ¬ï¼‰
            text_parts = [text.strip() for text in elem.stripped_strings if text.strip()]
            return "".join(text_parts)

        # 3. é€è¡Œè§£æï¼ˆé€‚é…colspanå’ŒåµŒå¥—æ ‡ç­¾ï¼‰
        extra_key_map = {
            "å†éƒ¨ç½²æ—¶é—´": "redployment_time",
            "åˆå§‹éƒ¨ç½²è´¹ç”¨": "initial_deployment_cost",
            "æ”»å‡»é—´éš”": "attack_interval",
            "é˜»æŒ¡æ•°": "block_count",
            "æ‰€å±åŠ¿åŠ›": "faction",
            "éšè—åŠ¿åŠ›": "hidden_faction"
        }

        for tr in extra_tbl.select("tr"):
            ths = tr.select("th")
            tds = tr.select("td")
            if not ths or not tds:
                continue  # è·³è¿‡æ— è¡¨å¤´/æ— å†…å®¹çš„è¡Œ

            # æå–<th>çš„çº¯æ–‡æœ¬ï¼ˆç§»é™¤åµŒå¥—çš„spanå›¾æ ‡ï¼‰
            th_text = get_pure_text(ths[0])
            if not th_text:
                continue

            # åŒ¹é…ç›®æ ‡å­—æ®µï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼Œåªè¦thæ–‡æœ¬åŒ…å«keyå°±ç»‘å®šï¼‰
            matched_field = None
            for map_key, field in extra_key_map.items():
                if map_key in th_text:
                    matched_field = field
                    break
            if not matched_field:
                continue

            # æå–<td>çš„çº¯æ–‡æœ¬ï¼ˆé€‚é…colspan=3çš„æƒ…å†µï¼‰
            td_text = get_pure_text(tds[0])
            if td_text:
                extra_attrs[matched_field] = td_text
                logger.debug(f"âœ… è§£æé¢å¤–å±æ€§ï¼š{th_text} â†’ {matched_field} = {td_text}")

        logger.debug(f"ğŸ“‹ è§£æåˆ°çš„é¢å¤–å±æ€§ï¼š{extra_attrs}")
        return {"base_attributes": base_attrs, "extra_attributes": extra_attrs}

    async def parse_chara(self):
        """è§£æå¹²å‘˜ç‰¹æ€§å’Œåˆ†æ”¯"""
        await self._get_soup()
        result = {
            "branch_name": "",
            "branch_description": "",
            "trait_details": ""
        }
        trait_tbl = self.soup.select_one("table.wikitable.logo")
        
        if trait_tbl:
            rows = trait_tbl.select("tr")
            # è§£æåˆ†æ”¯åç§°å’Œæè¿°
            if len(rows) > 1:
                tds = rows[1].find_all("td")
                result["branch_name"] = clean_text(tds[0]) if tds else ""
                result["branch_description"] = clean_text(tds[1]) if len(tds) > 1 else ""
            
            # è§£æåˆ†æ”¯è¯¦æƒ…
            branch_row = trait_tbl.find("tr", string=re.compile("åˆ†æ”¯ä¿¡æ¯"))
            if branch_row:
                next_row = branch_row.find_next_sibling("tr")
                if next_row:
                    result["trait_details"] = "".join(clean_desc(li) for li in next_row.select("li"))

        return result

    async def parse_talents(self):
        """è§£æå¹²å‘˜å¤©èµ‹"""
        await self._get_soup()
        talents = []
        talent_header = self.soup.find("span", id="å¤©èµ‹")
        if not talent_header:
            logger.debug("âš ï¸  æœªæ‰¾åˆ°å¤©èµ‹åŒºåŸŸ")
            return talents

        def parse_single_talent(table, talent_type: str, span_prefix: str) -> dict:
            """æå–å•ä¸ªå¤©èµ‹ï¼ˆå†…éƒ¨å·¥å…·å‡½æ•°ï¼‰"""
            talent = {
                "talent_type": talent_type,
                "talent_name": "",
                "remarks": "",
                "details": []
            }
            rows = table.find_all("tr")
            is_remark_section = False
            remark_text = ""

            for idx, row in enumerate(rows):
                if idx == 0:
                    continue  # è·³è¿‡è¡¨å¤´
                tds = row.find_all("td")
                th = row.find("th")

                # åˆ¤æ–­æ˜¯å¦ä¸ºå¤‡æ³¨è¡Œ
                if idx == len(rows) - 2 and th:
                    is_remark_section = True
                    continue
                if not tds:
                    continue

                # å¤„ç†å¤‡æ³¨
                if is_remark_section:
                    remark_text = clean_text(tds[0])
                    break

                # æå–å¤©èµ‹åç§°ï¼ˆä»…é¦–æ¬¡èµ‹å€¼ï¼‰
                current_name = clean_text(tds[0])
                if not talent["talent_name"] and current_name:
                    talent["talent_name"] = current_name

                # æå–å¤©èµ‹è¯¦æƒ…
                talent["details"].append({
                    "trigger_condition": clean_text(tds[1]),
                    "description": clean_desc(tds[2].select_one(f"span.{span_prefix}æ½œèƒ½_1")),
                    "potential_enhancement": clean_desc(tds[2].select_one(f"span.{span_prefix}æ½œèƒ½_2"))
                })

            talent["remarks"] = remark_text
            return talent if talent["talent_name"] and talent["details"] else None

        # è§£æç¬¬ä¸€å¤©èµ‹
        first_talent_tbl = talent_header.find_next("table", class_="wikitable")
        if first_talent_tbl:
            first_talent = parse_single_talent(first_talent_tbl, "ç¬¬ä¸€å¤©èµ‹", "ç¬¬ä¸€å¤©èµ‹")
            if first_talent:
                talents.append(first_talent)

        # è§£æç¬¬äºŒå¤©èµ‹
        second_talent_tbl = first_talent_tbl.find_next_sibling("table", class_="wikitable") if first_talent_tbl else None
        if second_talent_tbl:
            second_talent = parse_single_talent(second_talent_tbl, "ç¬¬äºŒå¤©èµ‹", "ç¬¬äºŒå¤©èµ‹")
            if second_talent:
                talents.append(second_talent)

        logger.debug(f"ğŸ“Š è§£æåˆ°å¤©èµ‹æ•°é‡ï¼š{len(talents)}")
        return talents

    async def parse_skills(self):
        """è§£æå¹²å‘˜æŠ€èƒ½ï¼ˆè¿˜åŸä½ æœ€åˆçš„ç®€æ´é€»è¾‘ï¼Œåªåšæœ€å°ä¿®å¤ï¼‰"""
        await self._get_soup()
        skills = []
        skill_header = self.soup.find("span", id="æŠ€èƒ½")
        
        if not skill_header:
            logger.debug("âš ï¸  æœªæ‰¾åˆ°æŠ€èƒ½åŒºåŸŸ")
            return skills

        # æå–å¯è§æ–‡æœ¬ï¼ˆä¿æŒä½ åŸæ¥çš„ç®€æ´ï¼‰
        def extract_visible_text(td_elem) -> str:
            visible_parts = []
            for child in td_elem.contents:
                if isinstance(child, str):
                    stripped = child.strip()
                    if stripped:
                        visible_parts.append(stripped)
                elif child.name == "span" and "display:none" not in child.get("style", ""):
                    span_text = clean_text(child)  # åªç”¨clean_textå…¼å®¹
                    if span_text:
                        visible_parts.append(span_text)
            return " ".join(visible_parts)

        # è§£æå•ä¸ªæŠ€èƒ½ï¼ˆä¿æŒä½ åŸæ¥çš„ç®€æ´ï¼ŒåªåŠ ç´¢å¼•é˜²æŠ¤ï¼‰
        def parse_single_skill(table, skill_idx: int) -> dict:
            skill = {
                "skill_number": skill_idx,
                "skill_name": "",
                "skill_type": "",
                "unlock_condition": f"ç²¾è‹±{skill_idx}",
                "remark": "",
                "skill_levels": []
            }
            rows = table.find_all("tr")
            is_remark = False

            for idx, row in enumerate(rows):
                tds = row.find_all("td")
                if not tds:  # æœ€å°é˜²æŠ¤ï¼šç©ºtdsç›´æ¥è·³è¿‡
                    continue

                if idx == 0:
                    # æœ€å°é˜²æŠ¤ï¼šç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
                    if len(tds) >= 2:
                        big_tag = tds[1].find("big")
                        skill["skill_name"] = clean_text(big_tag) if big_tag else clean_text(tds[1])
                    if len(tds) >= 3:
                        tooltip_spans = tds[2].find_all("span", class_="mc-tooltips")
                        skill["skill_type"] = "|".join([clean_text(span) for span in tooltip_spans])
                    continue

                # æå–å…³é”®ç­‰çº§ï¼ˆ7çº§å’Œä¸“ç²¾3ï¼‰
                if idx == 8 or idx == 11:
                    if len(tds) >= 5:
                        skill["skill_levels"].append({
                            "level": clean_text(tds[0]),
                            "description": extract_visible_text(tds[1]),
                            "initial_sp": clean_text(tds[2]),
                            "sp_cost": clean_text(tds[3]),
                            "duration": clean_text(tds[4])
                        })
                    continue

                # è¯†åˆ«å¤‡æ³¨è¡Œ
                if idx == len(rows) - 2 and row.find("th"):
                    is_remark = True
                    continue
                if is_remark:
                    skill["remark"] = clean_text(tds[0])
                    break

            return skill

        # ========== è¿˜åŸä½ æœ€åˆçš„å†™æ­»3æ¬¡å¾ªç¯ï¼ˆåªä¿®1ä¸ªé—®é¢˜ï¼‰ ==========
        current_table = skill_header.find_parent("h2").find_next_sibling("table")
        skill_tables = []
        for _ in range(3):
            # ä¿®å¤ï¼šæ”¾å®½è¡¨æ ¼classåˆ¤æ–­ï¼ˆåªéœ€è¦wikitableï¼Œä¸å¼ºåˆ¶nomobile logoï¼‰
            if current_table and "wikitable" in current_table.get("class", []):
                skill_tables.append(current_table)
                logger.debug(f"âœ… æ‰¾åˆ°ç¬¬{len(skill_tables)}ä¸ªæŠ€èƒ½è¡¨æ ¼")
                # ä¿®å¤ï¼šä¸‹ä¸€ä¸ªè¡¨æ ¼ä¹Ÿæ”¾å®½classåˆ¤æ–­
                current_table = current_table.find_next_sibling("table", class_=lambda c: c and "wikitable" in c)
            else:
                logger.debug(f"âš ï¸  æœªæ‰¾åˆ°ç¬¬{len(skill_tables)+1}ä¸ªæŠ€èƒ½è¡¨æ ¼")
                break

        # è§£ææŠ€èƒ½ï¼ˆä¿æŒç®€æ´ï¼‰
        for idx, table in enumerate(skill_tables, 1):
            skill = parse_single_skill(table, idx)
            if skill["skill_name"]:
                skills.append(skill)
                logger.debug(f"âœ… è§£ææŠ€èƒ½{idx}ï¼š{skill['skill_name']}")

        logger.debug(f"ğŸ“Š è§£æåˆ°æŠ€èƒ½æ•°é‡ï¼š{len(skills)}")
        return skills

    # ========== å…³é”®ä¿®æ”¹5ï¼šä¼˜åŒ–æœ¯è¯­æå–ï¼Œå‡å°‘èµ„æºæ¶ˆè€— ==========
    async def parse_terms(self):
        """è§£æå¹²å‘˜ç›¸å…³æœ¯è¯­ï¼ˆä¼˜åŒ–ï¼šé™åˆ¶æ•°é‡ã€æå‰æ£€æµ‹å´©æºƒï¼‰"""
        await self._get_soup()
        terms = []
        term_seen = set()
        total_success = 0
        total_failed = 0

        try:
            # 1. å®šä½æ ¸å¿ƒå†…å®¹åŒº
            content_div = self.soup.find("div", id="mw-content-text")
            if not content_div:
                logger.warning("âš ï¸  æœªæ‰¾åˆ°æ ¸å¿ƒå†…å®¹åŒºï¼Œè·³è¿‡æœ¯è¯­æå–")
                return terms

            # 2. ç­›é€‰æœ‰æ•ˆæœ¯è¯­æ ‡ç­¾
            term_tags = content_div.find_all(
                lambda tag: tag.name == "span"
                and tag.get("class")
                and any("mc-tooltips" in c for c in tag.get("class"))
                and len(clean_text(tag).strip()) >= self.term_min_length
                and not clean_text(tag).strip().isdigit()
            )
            total_terms = len(term_tags)
            logger.info(f"\nğŸ” æœ¯è¯­æå–å¼€å§‹ï¼šå…±æ‰¾åˆ° {total_terms} ä¸ªæœ‰æ•ˆæ½œåœ¨æœ¯è¯­æ ‡ç­¾")
            if total_terms == 0:
                return terms

            # 3. æ£€æŸ¥é¡µé¢çŠ¶æ€ï¼Œå¦‚æœé¡µé¢å·²å´©æºƒåˆ™æå‰é€€å‡º
            try:
                await self.page.evaluate("() => document.title")
            except Exception as e:
                logger.error("âŒ é¡µé¢å·²å´©æºƒï¼Œæ— æ³•è¿›è¡Œæœ¯è¯­æå–")
                return terms

            # 4. é™åˆ¶æœ€å¤§å¤„ç†æ•°é‡ï¼ˆä»50é™åˆ°20ï¼Œå‡å°‘èµ„æºæ¶ˆè€—ï¼‰
            max_terms = min(total_terms, 20)  # å…³é”®ï¼šé™åˆ¶æœ€å¤šå¤„ç†20ä¸ª
            processed_terms = 0

            # 5. é€ä¸ªå¤„ç†æœ¯è¯­
            for idx, term_tag in enumerate(term_tags, 1):
                if processed_terms >= max_terms:
                    logger.info(f"â­ï¸ å·²è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ {max_terms}ï¼Œåœæ­¢å¤„ç†")
                    break
                    
                term_name = clean_text(term_tag).strip()
                # è·³è¿‡é‡å¤æˆ–æ— æ•ˆæœ¯è¯­
                if not term_name or term_name in term_seen:
                    logger.info(f"â­ï¸  æœ¯è¯­{idx}/{total_terms}ï¼šè·³è¿‡ï¼ˆé‡å¤/æ— æ•ˆï¼‰â†’ åç§°ï¼š{term_name}")
                    continue

                try:
                    # 3.1 æ„å»ºCSSå®šä½å™¨
                    class_list = term_tag.get("class", [])
                    valid_classes = [c for c in class_list if "mc-tooltips" in c]
                    if not valid_classes:
                        logger.info(f"â­ï¸  æœ¯è¯­{idx}/{total_terms}ï¼šè·³è¿‡ï¼ˆæ— æœ‰æ•ˆclassï¼‰â†’ åç§°ï¼š{term_name}")
                        total_failed += 1
                        continue

                    term_class = valid_classes[0]
                    # å¤„ç†ç‰¹æ®Šå­—ç¬¦
                    safe_name = term_name.replace("'", "\\'").replace('"', '\\"').replace("\\", "\\\\")
                    css_selector = f"span.{term_class}:has-text('{safe_name}')"
                    locator = self.page.locator(css_selector).first

                    # è°ƒè¯•ï¼šæ‰“å°åŒ¹é…æ•°é‡
                    match_count = await self.page.locator(css_selector).count()
                    if match_count > 1:
                        logger.debug(f"âš ï¸  æœ¯è¯­{term_name}åŒ¹é…{match_count}ä¸ªå…ƒç´ ï¼Œå–ç¬¬ä¸€ä¸ª")
                        logger.info(f"âš ï¸  æœ¯è¯­{idx}/{total_terms}ï¼šå®šä½å™¨åŒ¹é…{match_count}ä¸ªå…ƒç´ ï¼Œå·²å–ç¬¬ä¸€ä¸ª â†’ åç§°ï¼š{term_name}")

                    # 3.2 æ‚¬æµ®è§¦å‘æç¤ºæ¡†ï¼ˆç¼©çŸ­ç­‰å¾…æ—¶é—´ï¼‰
                    await locator.wait_for(state="visible", timeout=self.timeouts["locator_wait"] or 10000)
                    await locator.scroll_into_view_if_needed()
                    await locator.hover(force=True)
                    await asyncio.sleep(self.wait_times["tooltip_render"] or 0.5)  # ç¼©çŸ­åˆ°0.5ç§’

                    # 3.3 æå–æç¤ºæ¡†å†…å®¹
                    term_type = "æ— "
                    term_desc = ""
                    tip_found = False

                    for tip_selector in self.tooltip_selectors:
                        tip_locator = self.page.locator(tip_selector).first
                        if await tip_locator.count() > 0:
                            tip_found = True
                            # æå–<strong>å†…å®¹ï¼ˆæœ¯è¯­ç±»å‹ï¼‰
                            strong_handles = await tip_locator.locator("strong").all()
                            strong_texts = []
                            for handle in strong_handles:
                                text = await handle.inner_text(timeout=self.timeouts["text_extract"] or 5000)
                                clean_text_val = text.strip().split(":")[0].rstrip("ï¼š:")
                                if clean_text_val:
                                    strong_texts.append(clean_text_val)
                            term_type = "ï¼Œ".join(strong_texts) if strong_texts else "æ— "
                            # é¿å…ç±»å‹ä¸åç§°é‡å¤
                            if term_type == term_name:
                                term_type = "æ— "

                            # æå–æ­£æ–‡ï¼ˆæ’é™¤strongï¼‰
                            content_handles = await tip_locator.locator(":not(strong)").all()
                            content_parts = []
                            for handle in content_handles:
                                text = await handle.inner_text(timeout=self.timeouts["text_extract"] or 5000)
                                clean_text_val = text.strip()
                                if clean_text_val:
                                    content_parts.append(clean_text_val)
                            term_desc = "\n".join(content_parts) if content_parts else ""

                            # æ­£æ–‡ä¸ºç©ºæ—¶å–å®Œæ•´æ–‡æœ¬
                            if not term_desc:
                                full_text = await tip_locator.inner_text(timeout=self.timeouts["text_extract"] or 5000)
                                if term_type != "æ— ":
                                    full_text = full_text.replace(f"{term_type}ï¼š", "").replace(f"{term_type}:", "").replace(term_type, "")
                                term_desc = full_text.strip()
                            break

                    if not tip_found:
                        logger.info(f"âŒ æœ¯è¯­{idx}/{total_terms}ï¼šå¤±è´¥ï¼ˆæœªæ‰¾åˆ°æç¤ºæ¡†ï¼‰â†’ åç§°ï¼š{term_name}")
                        total_failed += 1
                        continue

                    # 3.4 è¿‡æ»¤æ— æ•ˆæè¿°
                    formatted_desc = re.sub(r"\s+", "\n", term_desc).strip()
                    if len(formatted_desc) < self.desc_min_length:
                        logger.info(f"â­ï¸  æœ¯è¯­{idx}/{total_terms}ï¼šè·³è¿‡ï¼ˆæè¿°è¿‡çŸ­ï¼‰â†’ åç§°ï¼š{term_name}")
                        total_failed += 1
                        continue

                    # 3.5 åŠ å…¥ç»“æœï¼ˆå»é‡ï¼‰
                    if term_name not in term_seen:
                        terms.append({
                            "term_name": term_name,
                            "term_type": term_type,
                            "term_description": formatted_desc
                        })
                        term_seen.add(term_name)
                        total_success += 1
                        logger.info(f"âœ… æœ¯è¯­{idx}/{total_terms}ï¼šæˆåŠŸ â†’ åç§°ï¼š{term_name} | ç±»å‹ï¼š{term_type} | æè¿°é•¿åº¦ï¼š{len(formatted_desc)}å­—")

                    processed_terms += 1

                    # 3.6 æ¸…ç†çŠ¶æ€ï¼ˆç®€åŒ–ï¼Œå‡å°‘èµ„æºå ç”¨ï¼‰
                    try:
                        await self.page.mouse.move(100, 100)
                        await asyncio.sleep(0.1)  # ç¼©çŸ­åˆ°0.1ç§’
                    except Exception as e:
                        logger.warning(f"âš ï¸ é¼ æ ‡ç§»åŠ¨å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæœ¯è¯­: {str(e)[:30]}")

                except PlaywrightTimeoutError:
                    logger.info(f"âŒ æœ¯è¯­{idx}/{total_terms}ï¼šå¤±è´¥ï¼ˆè¶…æ—¶ï¼‰â†’ åç§°ï¼š{term_name}")
                    total_failed += 1
                    processed_terms += 1
                    continue
                except AttributeError as e:
                    logger.info(f"âŒ æœ¯è¯­{idx}/{total_terms}ï¼šå¤±è´¥ï¼ˆå±æ€§é”™è¯¯ï¼‰â†’ åç§°ï¼š{term_name} | é”™è¯¯ï¼š{str(e)[:50]}")
                    total_failed += 1
                    processed_terms += 1
                    continue
                except Exception as e:
                    error_msg = str(e).lower()
                    if "crashed" in error_msg or "target crashed" in error_msg:
                        logger.error(f"âŒ é¡µé¢å´©æºƒï¼Œåœæ­¢æœ¯è¯­æå–ï¼š{str(e)[:50]}")
                        break  # é¡µé¢å´©æºƒæ—¶ç«‹å³é€€å‡º
                    logger.info(f"âŒ æœ¯è¯­{idx}/{total_terms}ï¼šå¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰â†’ åç§°ï¼š{term_name} | é”™è¯¯ï¼š{str(e)[:50]}")
                    total_failed += 1
                    processed_terms += 1
                    continue

        except Exception as e:
            logger.error(f"âŒ æœ¯è¯­æå–ä¸»æµç¨‹é”™è¯¯ï¼š{str(e)}")

        # æœ€ç»ˆå»é‡ï¼ˆåŒé‡ä¿éšœï¼‰
        unique_terms = []
        final_seen = set()
        for term in terms:
            if term["term_name"] not in final_seen:
                final_seen.add(term["term_name"])
                unique_terms.append(term)

        # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
        logger.info(f"\nğŸ“Š æœ¯è¯­æå–å®Œæˆï¼šæ€»è®¡{total_terms}ä¸ªæœ‰æ•ˆæ½œåœ¨æœ¯è¯­ â†’ æˆåŠŸ{total_success}ä¸ª | å¤±è´¥{total_failed}ä¸ª | å»é‡å{len(unique_terms)}ä¸ª")
        return unique_terms

    async def parse_all(self):
        """æ•´åˆæ‰€æœ‰è§£æç»“æœ"""
        return {
            "operator_name": self.operator_name,
            "metadata": {
                "last_updated": datetime.now().isoformat(),
                "source": self.url,
                "version": "v10.0",
                "parser_config": {
                    "headless": self.headless,
                    "term_min_length": self.term_min_length,
                    "desc_min_length": self.desc_min_length
                }
            },
            "characteristic": await self.parse_chara(),
            "attributes": await self.parse_attrs(),
            "talents": await self.parse_talents(),
            "skills": await self.parse_skills(),
            "terms": await self.parse_terms()
        }

    async def save(self, result: dict):
        """ä¿å­˜å¹²å‘˜è¯¦æƒ…åˆ°JSON"""
        ensure_output_dir()
        safe_filename = clean_filename(self.operator_name)
        output_path = f"{JSON_OUTPUT_DIR}/{safe_filename}.json"
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æˆåŠŸä¿å­˜å¹²å‘˜è¯¦æƒ…: {output_path}")
        except IOError as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")

    # ========== å…³é”®ä¿®æ”¹6ï¼šé‡æ„runæ–¹æ³•ï¼Œä¼˜åŒ–èµ„æºé‡Šæ”¾ ==========
    async def run(self):
        """ä¸€é”®æ‰§è¡Œï¼šåˆå§‹åŒ–â†’è§£æâ†’ä¿å­˜ï¼ˆä¼˜åŒ–èµ„æºé‡Šæ”¾ï¼‰"""
        if not self.operator_name:
            logger.error("âŒ å¹²å‘˜åç§°ä¸ºç©ºï¼Œæ— æ³•è§£æ")
            return None

        logger.info(f"=== å¼€å§‹çˆ¬å–å¹²å‘˜: {self.operator_name} ({self.url}) ===")
        try:
            # åˆå§‹åŒ–é¡µé¢ï¼ˆå¤ç”¨å…¨å±€æµè§ˆå™¨ï¼‰
            await self._init_browser_page()
            # æ‰§è¡Œè§£æ
            result = await self.parse_all()
            # ä¿å­˜ç»“æœï¼ˆæ³¨é‡Šä¿ç•™ï¼‰
            # await self.save(result)

            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            logger.info("\n=== è§£æç»“æœæ±‡æ€» ===")
            logger.info(f"å¹²å‘˜åç§°: {result['operator_name']}")
            logger.info(f"åˆ†æ”¯åç§°: {result['characteristic']['branch_name']}")
            logger.info(f"å¤©èµ‹æ•°é‡: {len(result['talents'])}")
            logger.info(f"æŠ€èƒ½æ•°é‡: {len(result['skills'])}")
            logger.info(f"æœ¯è¯­æ•°é‡: {len(result['terms'])}")
            logger.info("====================")

            return result
        except PlaywrightTimeoutError:
            logger.error(f"âŒ é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆ{self.timeouts['page_load']/1000}ç§’ï¼‰")
            return None
        except Exception as e:
            logger.error(f"âŒ è§£æé”™è¯¯ï¼š{str(e)[:100]}")
            return None
        finally:
            # ========== å…³é”®ï¼šåªå…³é—­pageï¼Œä¸å…³é—­browserï¼ˆå…¨å±€å¤ç”¨ï¼‰ ==========
            if self.page:
                await self.page.close()
                self.page = None
                logger.info("ğŸ”Œ æµè§ˆå™¨é¡µé¢å·²å…³é—­ï¼ˆæµè§ˆå™¨å®ä¾‹å¤ç”¨ï¼‰")

# ä¿ç•™ç‹¬ç«‹æ‰§è¡Œå…¥å£ï¼ˆæ–¹ä¾¿å•ç‹¬è°ƒè¯•ï¼‰
if __name__ == "__main__":
    import sys
    operator_name = "ç„°å½±è‹‡è‰" if len(sys.argv) < 2 else sys.argv[1]
    # ç‹¬ç«‹è¿è¡Œæ—¶æ‰‹åŠ¨ç®¡ç†å…¨å±€æµè§ˆå™¨
    async def main():
        try:
            parser = OperatorDetailParser(operator_name)
            await parser.init_shared_browser()
            await parser.run()
        finally:
            await OperatorDetailParser.close_shared_browser()
    
    asyncio.run(main())