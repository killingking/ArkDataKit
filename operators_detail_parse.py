import asyncio
import json
import re
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
from config import BASE_URL, PLAYWRIGHT_CONFIG, JSON_OUTPUT_DIR
from utils import logger, clean_text, clean_desc, clean_filename, ensure_output_dir

class OperatorDetailParser:
    """å¹²å‘˜è¯¦æƒ…è§£æå™¨ï¼ˆæœ‰çŠ¶æ€ç±»å°è£…ï¼Œç»´æŠ¤page/soupï¼‰"""
    # ========== å…¨å±€å¤ç”¨çš„æµè§ˆå™¨/ä¸Šä¸‹æ–‡ï¼ˆç±»å±æ€§ï¼‰ ==========
    _shared_playwright = None
    _shared_browser = None
    _shared_context = None
    _browser_initialized = False
    _lock = asyncio.Lock()  # æ–°å¢ï¼šå¹¶å‘é”ï¼Œé¿å…å¤šå®ä¾‹ç«äº‰èµ„æº

    # ========== 1. åˆå§‹åŒ–æ–¹æ³• ==========
    def __init__(self, operator_name: str):
        self.operator_name = operator_name.strip()
        self.url = f"{BASE_URL}/w/{self.operator_name}" if self.operator_name else ""
        self.page = None
        self.soup = None
        
        # ä»é…ç½®è¯»å–å‚æ•°
        self.term_min_length = PLAYWRIGHT_CONFIG["term_filter"]["min_length"]
        self.desc_min_length = PLAYWRIGHT_CONFIG["term_filter"]["desc_min_length"]
        self.tooltip_selectors = PLAYWRIGHT_CONFIG["tooltip_selectors"]
        self.wait_times = PLAYWRIGHT_CONFIG["wait_time"]
        self.timeouts = PLAYWRIGHT_CONFIG["timeout"]
        self.browser_args = PLAYWRIGHT_CONFIG["browser_args"]
        self.headless = PLAYWRIGHT_CONFIG["headless"]

    # ========== 2. å…¨å±€æµè§ˆå™¨åˆå§‹åŒ–ï¼ˆåŠ é”+å±æ€§æ£€æŸ¥ï¼‰ ==========
    @classmethod
    async def init_shared_browser(cls):
        """åˆå§‹åŒ–å…¨å±€å¤ç”¨çš„æµè§ˆå™¨å®ä¾‹ï¼ˆåŠ é”+çŠ¶æ€é˜²æŠ¤ï¼‰"""
        async with cls._lock:  # å…³é”®ï¼šå¹¶å‘å®‰å…¨
            if cls._browser_initialized:
                # åŒé‡æ£€æŸ¥ï¼šå¯¹è±¡å­˜åœ¨ + æœ‰is_closedæ–¹æ³• + æœªå…³é—­
                context_valid = (
                    cls._shared_context 
                    and hasattr(cls._shared_context, 'is_closed') 
                    and not cls._shared_context.is_closed()
                )
                if context_valid:
                    return cls._shared_context
                else:
                    logger.warning("âš ï¸ å…¨å±€ä¸Šä¸‹æ–‡æ— æ•ˆï¼Œæ¸…ç†åé‡æ–°åˆå§‹åŒ–")
                    await cls.close_shared_browser()

            try:
                cls._shared_playwright = await async_playwright().start()
                browser_args = PLAYWRIGHT_CONFIG["browser_args"] + [
                    "--no-sandbox",
                    "--disable-gpu",
                    "--disable-dev-shm-usage",
                    "--disk-cache-dir=/tmp/playwright-cache",
                    "--max-old-space-size=256",
                    "--memory-pressure-off"
                ]
                cls._shared_browser = await cls._shared_playwright.chromium.launch(
                    headless=cls.headless,
                    args=browser_args,
                    timeout=60000
                )
                cls._shared_context = await cls._shared_browser.new_context(
                    viewport={"width": 1920, "height": 1080},
                    user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
                cls._browser_initialized = True
                logger.info("âœ… å…¨å±€æµè§ˆå™¨å®ä¾‹åˆå§‹åŒ–å®Œæˆï¼ˆå¤ç”¨æ¨¡å¼ï¼‰")
                return cls._shared_context
            except Exception as e:
                logger.error(f"âŒ å…¨å±€æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
                await cls.close_shared_browser()
                raise

    # ========== 3. å…¨å±€æµè§ˆå™¨å…³é—­ï¼ˆåŠ é”+å±æ€§æ£€æŸ¥ï¼‰ ==========
    @classmethod
    async def close_shared_browser(cls):
        """å…³é—­å…¨å±€æµè§ˆå™¨å®ä¾‹ï¼ˆåŠ é”+å®‰å…¨å…³é—­ï¼‰"""
        async with cls._lock:
            # å…³é—­ä¸Šä¸‹æ–‡ï¼ˆæ£€æŸ¥å¯¹è±¡+æ–¹æ³•æ˜¯å¦å­˜åœ¨ï¼‰
            if cls._shared_context and hasattr(cls._shared_context, 'is_closed') and not cls._shared_context.is_closed():
                try:
                    await cls._shared_context.close()
                except Exception as e:
                    logger.warning(f"âš ï¸ å…³é—­ä¸Šä¸‹æ–‡æ—¶è­¦å‘Šï¼š{str(e)}")
            cls._shared_context = None

            # å…³é—­æµè§ˆå™¨
            if cls._shared_browser and hasattr(cls._shared_browser, 'is_closed') and not cls._shared_browser.is_closed():
                try:
                    await cls._shared_browser.close()
                except Exception as e:
                    logger.warning(f"âš ï¸ å…³é—­æµè§ˆå™¨æ—¶è­¦å‘Šï¼š{str(e)}")
            cls._shared_browser = None

            # åœæ­¢playwright
            if cls._shared_playwright:
                try:
                    await cls._shared_playwright.stop()
                except Exception as e:
                    logger.warning(f"âš ï¸ åœæ­¢Playwrightæ—¶è­¦å‘Šï¼š{str(e)}")
            cls._shared_playwright = None

            cls._browser_initialized = False
            logger.info("ğŸ”Œ å…¨å±€æµè§ˆå™¨å®ä¾‹å·²å…³é—­")

    # ========== 4. é¡µé¢åˆå§‹åŒ–ï¼ˆå±æ€§æ£€æŸ¥+å¼‚å¸¸é˜²æŠ¤ï¼‰ ==========
    async def _init_browser_page(self):
        """å†…éƒ¨æ–¹æ³•ï¼šåˆå§‹åŒ–é¡µé¢ï¼ˆå®‰å…¨é˜²æŠ¤ï¼‰"""
        if not self.operator_name:
            raise ValueError("âŒ å¹²å‘˜åç§°ä¸èƒ½ä¸ºç©º")
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ï¼ˆåŠ é”ç¡®ä¿å®‰å…¨ï¼‰
                async with self._lock:
                    context = await self.init_shared_browser()
                    # æ£€æŸ¥ä¸Šä¸‹æ–‡æœ‰æ•ˆæ€§
                    context_valid = (
                        context 
                        and hasattr(context, 'is_closed') 
                        and not context.is_closed()
                    )
                    if not context_valid:
                        raise Exception("å…¨å±€ä¸Šä¸‹æ–‡æ— æ•ˆ")

                # å…³é—­æ—§é¡µé¢ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
                if self.page and hasattr(self.page, 'is_closed') and not self.page.is_closed():
                    await self.page.close()
                self.page = await context.new_page()
                
                # è¶…æ—¶é…ç½®
                self.page.set_default_timeout(self.timeouts["page_load"] or 60000)
                self.page.set_default_navigation_timeout(self.timeouts["page_load"] or 60000)
                
                # åŠ è½½é¡µé¢
                await self.page.goto(
                    self.url, 
                    wait_until="load",
                    timeout=60000
                )
                await self.page.wait_for_selector("#mw-content-text", timeout=60000)
                await self.page.wait_for_load_state("networkidle")
                await asyncio.sleep(1)
                logger.info(f"âœ… æµè§ˆå™¨é¡µé¢åˆå§‹åŒ–å®Œæˆï¼š{self.url}")
                return None
                
            except Exception as e:
                error_msg = str(e).lower()
                # å¤„ç†å´©æºƒåœºæ™¯
                if "closed" in error_msg or "crashed" in error_msg or "target" in error_msg:
                    logger.error(f"âŒ æµè§ˆå™¨/ä¸Šä¸‹æ–‡å¼‚å¸¸ï¼Œå°è¯•é‡å¯ï¼ˆ{attempt+1}/{max_retries}ï¼‰ï¼š{str(e)[:50]}")
                    await self.close_shared_browser()
                    await asyncio.sleep(5)
                
                if attempt == max_retries - 1:
                    raise Exception(f"âŒ é¡µé¢åˆå§‹åŒ–å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {str(e)}")
                
                logger.warning(f"âš ï¸ é¡µé¢åˆå§‹åŒ–å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt + 1}/{max_retries}): {str(e)}")
                # å…³é—­å½“å‰é¡µé¢
                if self.page and hasattr(self.page, 'is_closed') and not self.page.is_closed():
                    await self.page.close()
                self.page = None
                await asyncio.sleep(3)

    # ========== ä»¥ä¸‹æ–¹æ³•ä¿æŒä¸å˜ï¼Œä»…å¤åˆ¶åŸæœ‰ä»£ç  ==========
    async def _get_soup(self):
        """å†…éƒ¨æ–¹æ³•ï¼šå¤ç”¨soupå¯¹è±¡ï¼ˆé¿å…é‡å¤è§£æé¡µé¢ï¼‰"""
        if not self.soup and self.page:
            content = await self.page.content()
            self.soup = BeautifulSoup(content, "lxml")
        return self.soup

    async def parse_attrs(self):
        """è§£æå¹²å‘˜å±æ€§"""
        await self._get_soup()
        base_attrs = {
            "elite_0_level_1": {},
            "elite_0_max": {},
            "elite_1_max": {},
            "elite_2_max": {},
            "trust_bonus": {}
        }
        base_tbl = self.soup.select_one("table.char-base-attr-table")
        
        if base_tbl:
            headers = [clean_text(th) for th in base_tbl.select("tr:first-child th, tr:first-child td")]
            key_mapping = [
                "elite_0_level_1" if "ç²¾è‹±0 1çº§" in h else
                "elite_0_max" if "ç²¾è‹±0 æ»¡çº§" in h else
                "elite_1_max" if "ç²¾è‹±1 æ»¡çº§" in h else
                "elite_2_max" if "ç²¾è‹±2 æ»¡çº§" in h else
                "trust_bonus" if "ä¿¡èµ–åŠ æˆä¸Šé™" in h else
                "" for h in headers
            ]
            attr_mapping = {
                "ç”Ÿå‘½ä¸Šé™": "max_hp",
                "æ”»å‡»": "atk",
                "é˜²å¾¡": "def",
                "æ³•æœ¯æŠ—æ€§": "res"
            }
            
            for tr in base_tbl.select("tr")[1:]:
                tds = [clean_text(td) for td in tr.select("th, td")]
                if len(tds) < 2:
                    continue
                attr_key = attr_mapping.get(tds[0], tds[0].lower())
                for idx, val in enumerate(tds[1:], 1):
                    if idx < len(key_mapping) and key_mapping[idx]:
                        base_attrs[key_mapping[idx]][attr_key] = val

        extra_attrs = {}
        extra_tbl = self.soup.select_one("table.char-extra-attr-table")
        if extra_tbl:
            extra_key_map = {
                "å†éƒ¨ç½²æ—¶é—´": "redployment_time",
                "åˆå§‹éƒ¨ç½²è´¹ç”¨": "initial_deployment_cost",
                "æ”»å‡»é—´éš”": "attack_interval",
                "é˜»æŒ¡æ•°": "block_count",
                "æ‰€å±åŠ¿åŠ›": "faction",
                "éšè—åŠ¿åŠ›": "hidden_faction"
            }
            for tr in extra_tbl.select("tr"):
                ths = tr.select("th")
                tds = tr.select("td")
                if not ths or not tds:
                    continue
                th_text = clean_text(ths[0])
                th_text = th_text.replace('"', '').replace('â€œ', '').replace('â€', '').strip()
                td_text = clean_text(tds[0])
                
                if th_text in extra_key_map:
                    extra_attrs[extra_key_map[th_text]] = td_text
                    logger.debug(
                        f"âœ… è§£æé¢å¤–å±æ€§ï¼š{th_text} â†’ {extra_key_map[th_text]} = {td_text}"
                    )

        logger.debug(f"ğŸ“‹ è§£æåˆ°çš„é¢å¤–å±æ€§ï¼š{extra_attrs}")
        return {
            "base_attributes": base_attrs,
            "extra_attributes": extra_attrs
        } 

    async def parse_chara(self):
        """è§£æå¹²å‘˜ç‰¹æ€§å’Œåˆ†æ”¯"""
        await self._get_soup()
        result = {
            "branch_name": "",
            "branch_description": "",
            "trait_details": ""
        }
        trait_tbl = self.soup.select_one("table.wikitable.logo")
        
        if trait_tbl:
            rows = trait_tbl.select("tr")
            if len(rows) > 1:
                tds = rows[1].find_all("td")
                result["branch_name"] = clean_text(tds[0]) if tds else ""
                result["branch_description"] = clean_text(tds[1]) if len(tds) > 1 else ""
            
            branch_row = trait_tbl.find("tr", string=re.compile("åˆ†æ”¯ä¿¡æ¯"))
            if branch_row:
                next_row = branch_row.find_next_sibling("tr")
                if next_row:
                    result["trait_details"] = "".join(clean_desc(li) for li in next_row.select("li"))

        return result

    async def parse_talents(self):
        """è§£æå¹²å‘˜å¤©èµ‹"""
        await self._get_soup()
        talents = []
        talent_header = self.soup.find("span", id="å¤©èµ‹")
        if not talent_header:
            logger.debug("âš ï¸  æœªæ‰¾åˆ°å¤©èµ‹åŒºåŸŸ")
            return talents

        def parse_single_talent(table, talent_type: str, span_prefix: str) -> dict:
            talent = {
                "talent_type": talent_type,
                "talent_name": "",
                "remarks": "",
                "details": []
            }
            rows = table.find_all("tr")
            is_remark_section = False
            remark_text = ""

            for idx, row in enumerate(rows):
                if idx == 0:
                    continue
                tds = row.find_all("td")
                th = row.find("th")

                if idx == len(rows) - 2 and th:
                    is_remark_section = True
                    continue
                if not tds:
                    continue

                if is_remark_section:
                    remark_text = clean_text(tds[0])
                    break

                current_name = clean_text(tds[0])
                if not talent["talent_name"] and current_name:
                    talent["talent_name"] = current_name

                talent["details"].append({
                    "trigger_condition": clean_text(tds[1]),
                    "description": clean_desc(tds[2].select_one(f"span.{span_prefix}æ½œèƒ½_1")),
                    "potential_enhancement": clean_desc(tds[2].select_one(f"span.{span_prefix}æ½œèƒ½_2"))
                })

            talent["remarks"] = remark_text
            return talent if talent["talent_name"] and talent["details"] else None

        first_talent_tbl = talent_header.find_next("table", class_="wikitable")
        if first_talent_tbl:
            first_talent = parse_single_talent(first_talent_tbl, "ç¬¬ä¸€å¤©èµ‹", "ç¬¬ä¸€å¤©èµ‹")
            if first_talent:
                talents.append(first_talent)

        second_talent_tbl = first_talent_tbl.find_next_sibling("table", class_="wikitable") if first_talent_tbl else None
        if second_talent_tbl:
            second_talent = parse_single_talent(second_talent_tbl, "ç¬¬äºŒå¤©èµ‹", "ç¬¬äºŒå¤©èµ‹")
            if second_talent:
                talents.append(second_talent)

        logger.debug(f"ğŸ“Š è§£æåˆ°å¤©èµ‹æ•°é‡ï¼š{len(talents)}")
        return talents

    async def parse_skills(self):
        """è§£æå¹²å‘˜æŠ€èƒ½"""
        await self._get_soup()
        skills = []
        skill_header = self.soup.find("span", id="æŠ€èƒ½")
        
        if not skill_header:
            logger.debug("âš ï¸  æœªæ‰¾åˆ°æŠ€èƒ½åŒºåŸŸ")
            return skills

        def extract_visible_text(td_elem) -> str:
            visible_parts = []
            for child in td_elem.contents:
                if isinstance(child, str):
                    stripped = child.strip()
                    if stripped:
                        visible_parts.append(stripped)
                elif child.name == "span" and "display:none" not in child.get("style", ""):
                    span_text = clean_text(child)
                    if span_text:
                        visible_parts.append(span_text)
            return " ".join(visible_parts)

        def parse_single_skill(table, skill_idx: int) -> dict:
            skill = {
                "skill_number": skill_idx,
                "skill_name": "",
                "skill_type": "",
                "unlock_condition": f"ç²¾è‹±{skill_idx}",
                "remark": "",
                "skill_levels": []
            }
            rows = table.find_all("tr")
            is_remark = False

            for idx, row in enumerate(rows):
                tds = row.find_all("td")
                if not tds:
                    continue

                if idx == 0:
                    if len(tds) >= 2:
                        big_tag = tds[1].find("big")
                        skill["skill_name"] = clean_text(big_tag) if big_tag else clean_text(tds[1])
                    if len(tds) >= 3:
                        tooltip_spans = tds[2].find_all("span", class_="mc-tooltips")
                        skill["skill_type"] = "|".join([clean_text(span) for span in tooltip_spans])
                    continue

                if idx == 8 or idx == 11:
                    if len(tds) >= 5:
                        skill["skill_levels"].append({
                            "level": clean_text(tds[0]),
                            "description": extract_visible_text(tds[1]),
                            "initial_sp": clean_text(tds[2]),
                            "sp_cost": clean_text(tds[3]),
                            "duration": clean_text(tds[4])
                        })
                    continue

                if idx == len(rows) - 2 and row.find("th"):
                    is_remark = True
                    continue
                if is_remark:
                    skill["remark"] = clean_text(tds[0])
                    break

            return skill

        skill_h2 = skill_header.find_parent("h2")
        if not skill_h2:
            logger.debug("âš ï¸  æœªæ‰¾åˆ°æŠ€èƒ½åŒºåŸŸçš„H2æ ‡ç­¾ï¼Œè·³è¿‡æŠ€èƒ½è§£æ")
            return skills
        
        skill_no = skill_h2.find_next_sibling("p")
        skill_tables = []

        for i in range(1, 4):
            if not skill_no:
                logger.debug(f"âš ï¸  æœªæ‰¾åˆ°ç¬¬{i}ä¸ªæŠ€èƒ½è¡¨æ ¼çš„é”šç‚¹Pæ ‡ç­¾ï¼Œç»ˆæ­¢æŸ¥æ‰¾")
                break

            if clean_text(skill_no).find("æŠ€èƒ½") > -1:
                current_table = skill_no.find_next_sibling("table")
                if current_table and all(cls in current_table.get("class", []) for cls in ["wikitable", "nomobile", "logo"]):
                    skill_tables.append(current_table)
                    logger.debug(f"âœ… æ‰¾åˆ°ç¬¬{i}ä¸ªæŠ€èƒ½è¡¨æ ¼")
                    skill_no = skill_no.find_next_sibling("p")
                else:
                    logger.debug(f"âš ï¸  ç¬¬{i}ä¸ªæŠ€èƒ½è¡¨æ ¼classä¸åŒ¹é…ï¼Œè·³è¿‡")
                    skill_no = skill_no.find_next_sibling("p")
            else:
                logger.debug(f"âš ï¸  ç¬¬{i}ä¸ªæŠ€èƒ½è¡¨æ ¼çš„Pæ ‡ç­¾ä¸å«â€œæŠ€èƒ½â€ï¼Œç»ˆæ­¢æŸ¥æ‰¾")
                break

        logger.debug(f"ğŸ“Š æŠ€èƒ½è¡¨æ ¼æŸ¥æ‰¾å®Œæˆï¼šå…±æ‰¾åˆ° {len(skill_tables)} ä¸ªæœ‰æ•ˆè¡¨æ ¼")

        for idx, table in enumerate(skill_tables, 1):
            skill = parse_single_skill(table, idx)
            if skill["skill_name"]:
                skills.append(skill)
                logger.debug(f"âœ… è§£ææŠ€èƒ½{idx}ï¼š{skill['skill_name']}")

        logger.debug(f"ğŸ“Š è§£æåˆ°æŠ€èƒ½æ•°é‡ï¼š{len(skills)}")
        return skills

    async def parse_terms(self):
        """è§£æå¹²å‘˜ç›¸å…³æœ¯è¯­"""
        await self._get_soup()
        terms = []
        term_seen = set()
        total_success = 0
        total_failed = 0

        try:
            content_div = self.soup.find("div", id="mw-content-text")
            if not content_div:
                logger.warning("âš ï¸  æœªæ‰¾åˆ°æ ¸å¿ƒå†…å®¹åŒºï¼Œè·³è¿‡æœ¯è¯­æå–")
                return terms

            term_tags = content_div.find_all(
                lambda tag: tag.name == "span"
                and tag.get("class")
                and any("mc-tooltips" in c for c in tag.get("class"))
                and len(clean_text(tag).strip()) >= self.term_min_length
                and not clean_text(tag).strip().isdigit()
            )
            total_terms = len(term_tags)
            logger.info(f"\nğŸ” æœ¯è¯­æå–å¼€å§‹ï¼šå…±æ‰¾åˆ° {total_terms} ä¸ªæœ‰æ•ˆæ½œåœ¨æœ¯è¯­æ ‡ç­¾")
            if total_terms == 0:
                return terms

            try:
                await self.page.evaluate("() => document.title")
            except Exception as e:
                logger.error(f"âŒ é¡µé¢çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡æœ¯è¯­æå–ï¼š{str(e)[:50]}")
                return terms

            max_terms = min(total_terms, 20)
            processed_terms = 0

            for idx, term_tag in enumerate(term_tags, 1):
                if processed_terms >= max_terms:
                    logger.info(f"â­ï¸ å·²è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ {max_terms}ï¼Œåœæ­¢å¤„ç†")
                    break
                    
                term_name = clean_text(term_tag).strip()
                if not term_name or term_name in term_seen:
                    logger.info(f"â­ï¸  æœ¯è¯­{idx}/{total_terms}ï¼šè·³è¿‡ï¼ˆé‡å¤/æ— æ•ˆï¼‰â†’ åç§°ï¼š{term_name}")
                    continue

                try:
                    class_list = term_tag.get("class", [])
                    valid_classes = [c for c in class_list if "mc-tooltips" in c]
                    if not valid_classes:
                        logger.info(f"â­ï¸  æœ¯è¯­{idx}/{total_terms}ï¼šè·³è¿‡ï¼ˆæ— æœ‰æ•ˆclassï¼‰â†’ åç§°ï¼š{term_name}")
                        total_failed += 1
                        continue

                    term_class = valid_classes[0]
                    safe_name = term_name.replace("'", "\\'").replace('"', '\\"').replace("\\", "\\\\")
                    css_selector = f"span.{term_class}:has-text('{safe_name}')"
                    locator = self.page.locator(css_selector).first

                    match_count = await self.page.locator(css_selector).count()
                    if match_count > 1:
                        logger.debug(f"âš ï¸  æœ¯è¯­{term_name}åŒ¹é…{match_count}ä¸ªå…ƒç´ ï¼Œå–ç¬¬ä¸€ä¸ª")
                        logger.info(f"âš ï¸  æœ¯è¯­{idx}/{total_terms}ï¼šå®šä½å™¨åŒ¹é…{match_count}ä¸ªå…ƒç´  â†’ åç§°ï¼š{term_name}")

                    await locator.wait_for(state="visible", timeout=self.timeouts["locator_wait"] or 10000)
                    await locator.scroll_into_view_if_needed()
                    await locator.hover(force=True)
                    await asyncio.sleep(self.wait_times["tooltip_render"] or 0.5)

                    term_type = "æ— "
                    term_desc = ""
                    tip_found = False

                    for tip_selector in self.tooltip_selectors:
                        tip_locator = self.page.locator(tip_selector).first
                        if await tip_locator.count() > 0:
                            tip_found = True
                            strong_handles = await tip_locator.locator("strong").all()
                            strong_texts = []
                            for handle in strong_handles:
                                text = await handle.inner_text(timeout=self.timeouts["text_extract"] or 5000)
                                clean_text_val = text.strip().split(":")[0].rstrip("ï¼š:")
                                if clean_text_val:
                                    strong_texts.append(clean_text_val)
                            term_type = "ï¼Œ".join(strong_texts) if strong_texts else "æ— "
                            if term_type == term_name:
                                term_type = "æ— "

                            content_handles = await tip_locator.locator(":not(strong)").all()
                            content_parts = []
                            for handle in content_handles:
                                text = await handle.inner_text(timeout=self.timeouts["text_extract"] or 5000)
                                clean_text_val = text.strip()
                                if clean_text_val:
                                    content_parts.append(clean_text_val)
                            term_desc = "\n".join(content_parts) if content_parts else ""

                            if not term_desc:
                                full_text = await tip_locator.inner_text(timeout=self.timeouts["text_extract"] or 5000)
                                if term_type != "æ— ":
                                    full_text = full_text.replace(f"{term_type}ï¼š", "").replace(f"{term_type}:", "").replace(term_type, "")
                                term_desc = full_text.strip()
                            break

                    if not tip_found:
                        logger.info(f"âŒ æœ¯è¯­{idx}/{total_terms}ï¼šå¤±è´¥ï¼ˆæœªæ‰¾åˆ°æç¤ºæ¡†ï¼‰â†’ åç§°ï¼š{term_name}")
                        total_failed += 1
                        continue

                    formatted_desc = re.sub(r"\s+", "\n", term_desc).strip()
                    if len(formatted_desc) < self.desc_min_length:
                        logger.info(f"â­ï¸  æœ¯è¯­{idx}/{total_terms}ï¼šè·³è¿‡ï¼ˆæè¿°è¿‡çŸ­ï¼‰â†’ åç§°ï¼š{term_name}")
                        total_failed += 1
                        continue

                    if term_name not in term_seen:
                        terms.append({
                            "term_name": term_name,
                            "term_type": term_type,
                            "term_description": formatted_desc
                        })
                        term_seen.add(term_name)
                        total_success += 1
                        logger.info(f"âœ… æœ¯è¯­{idx}/{total_terms}ï¼šæˆåŠŸ â†’ åç§°ï¼š{term_name} | ç±»å‹ï¼š{term_type} | æè¿°é•¿åº¦ï¼š{len(formatted_desc)}å­—")

                    processed_terms += 1

                    try:
                        await self.page.mouse.move(100, 100)
                        await asyncio.sleep(0.1)
                    except Exception as e:
                        logger.warning(f"âš ï¸ é¼ æ ‡ç§»åŠ¨å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæœ¯è¯­: {str(e)[:30]}")

                except PlaywrightTimeoutError:
                    logger.info(f"âŒ æœ¯è¯­{idx}/{total_terms}ï¼šå¤±è´¥ï¼ˆè¶…æ—¶ï¼‰â†’ åç§°ï¼š{term_name}")
                    total_failed += 1
                    processed_terms += 1
                    continue
                except AttributeError as e:
                    logger.info(f"âŒ æœ¯è¯­{idx}/{total_terms}ï¼šå¤±è´¥ï¼ˆå±æ€§é”™è¯¯ï¼‰â†’ åç§°ï¼š{term_name} | é”™è¯¯ï¼š{str(e)[:50]}")
                    total_failed += 1
                    processed_terms += 1
                    continue
                except Exception as e:
                    error_msg = str(e).lower()
                    if "crashed" in error_msg or "target crashed" in error_msg:
                        logger.error(f"âŒ é¡µé¢å´©æºƒï¼Œåœæ­¢æœ¯è¯­æå–ï¼š{str(e)[:50]}")
                        break
                    logger.info(f"âŒ æœ¯è¯­{idx}/{total_terms}ï¼šå¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰â†’ åç§°ï¼š{term_name} | é”™è¯¯ï¼š{str(e)[:50]}")
                    total_failed += 1
                    processed_terms += 1
                    continue

        except Exception as e:
            logger.error(f"âŒ æœ¯è¯­æå–ä¸»æµç¨‹é”™è¯¯ï¼š{str(e)}")

        unique_terms = []
        final_seen = set()
        for term in terms:
            if term["term_name"] not in final_seen:
                final_seen.add(term["term_name"])
                unique_terms.append(term)

        logger.info(f"\nğŸ“Š æœ¯è¯­æå–å®Œæˆï¼šæ€»è®¡{total_terms}ä¸ªæœ‰æ•ˆæ½œåœ¨æœ¯è¯­ â†’ æˆåŠŸ{total_success}ä¸ª | å¤±è´¥{total_failed}ä¸ª | å»é‡å{len(unique_terms)}ä¸ª")
        return unique_terms

    async def parse_all(self):
        """æ•´åˆæ‰€æœ‰è§£æç»“æœ"""
        return {
            "operator_name": self.operator_name,
            "metadata": {
                "last_updated": datetime.now().isoformat(),
                "source": self.url,
                "version": "v10.0",
                "parser_config": {
                    "headless": self.headless,
                    "term_min_length": self.term_min_length,
                    "desc_min_length": self.desc_min_length
                }
            },
            "characteristic": await self.parse_chara(),
            "attributes": await self.parse_attrs(),
            "talents": await self.parse_talents(),
            "skills": await self.parse_skills(),
            "terms": await self.parse_terms()
        }

    async def save(self, result: dict):
        """ä¿å­˜å¹²å‘˜è¯¦æƒ…åˆ°JSON"""
        ensure_output_dir()
        safe_filename = clean_filename(self.operator_name)
        output_path = f"{JSON_OUTPUT_DIR}/{safe_filename}.json"
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æˆåŠŸä¿å­˜å¹²å‘˜è¯¦æƒ…: {output_path}")
        except IOError as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")

    async def run(self):
        """ä¸€é”®æ‰§è¡Œï¼šåˆå§‹åŒ–â†’è§£æâ†’ä¿å­˜"""
        if not self.operator_name:
            logger.error("âŒ å¹²å‘˜åç§°ä¸ºç©ºï¼Œæ— æ³•è§£æ")
            return None

        logger.info(f"=== å¼€å§‹çˆ¬å–å¹²å‘˜: {self.operator_name} ({self.url}) ===")
        try:
            await self._init_browser_page()
            result = await self.parse_all()
            # await self.save(result)

            logger.info("\n=== è§£æç»“æœæ±‡æ€» ===")
            logger.info(f"å¹²å‘˜åç§°: {result['operator_name']}")
            logger.info(f"åˆ†æ”¯åç§°: {result['characteristic']['branch_name']}")
            logger.info(f"å¤©èµ‹æ•°é‡: {len(result['talents'])}")
            logger.info(f"æŠ€èƒ½æ•°é‡: {len(result['skills'])}")
            logger.info(f"æœ¯è¯­æ•°é‡: {len(result['terms'])}")
            logger.info("====================")

            return result
        except PlaywrightTimeoutError:
            logger.error(f"âŒ é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆ{self.timeouts['page_load']/1000}ç§’ï¼‰")
            return None
        except Exception as e:
            logger.error(f"âŒ è§£æé”™è¯¯ï¼š{str(e)[:100]}")
            return None
        finally:
            # å®‰å…¨å…³é—­é¡µé¢
            if self.page and hasattr(self.page, 'is_closed') and not self.page.is_closed():
                await self.page.close()
                self.page = None
                logger.info("ğŸ”Œ æµè§ˆå™¨é¡µé¢å·²å…³é—­ï¼ˆæµè§ˆå™¨å®ä¾‹å¤ç”¨ï¼‰")

if __name__ == "__main__":
    import sys
    operator_name = "ç„°å½±è‹‡è‰" if len(sys.argv) < 2 else sys.argv[1]
    
    async def main():
        try:
            parser = OperatorDetailParser(operator_name)
            await parser.init_shared_browser()
            await parser.run()
        finally:
            await OperatorDetailParser.close_shared_browser()
    
    asyncio.run(main())