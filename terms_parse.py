# terms_parse.py
import requests
import json
from bs4 import BeautifulSoup
from config import TERM_STATIC_URL, HEADERS, JSON_OUTPUT_DIR
from utils import logger, clean_text, deduplicate_terms, ensure_output_dir

class TermStaticCrawler:
    """é™æ€æœ¯è¯­çˆ¬å–å™¨ï¼ˆè½»é‡ç±»å°è£…ï¼Œæ— çŠ¶æ€ï¼‰"""
    def __init__(self):
        # æ— çŠ¶æ€ï¼Œä»…åˆå§‹åŒ–é…ç½®å¼•ç”¨
        self.url = TERM_STATIC_URL
        self.headers = HEADERS
        self.output_dir = JSON_OUTPUT_DIR
        self.output_filename = "prts_terms.json"

    def fetch(self) -> str:
        """æŠ“å–æœ¯è¯­é¡µé¢HTML"""
        try:
            response = requests.get(self.url, headers=self.headers, timeout=30)
            response.raise_for_status()  # æ•è·HTTPé”™è¯¯
            response.encoding = "utf-8"
            logger.info(f"âœ… æˆåŠŸæŠ“å–æœ¯è¯­é¡µé¢ï¼š{self.url}")
            return response.text
        except Exception as e:
            logger.error(f"âŒ æŠ“å–å¤±è´¥ï¼š{type(e).__name__}: {str(e)}")
            raise

    def parse(self, html: str) -> list[dict]:
        """è§£ææœ¯è¯­æ•°æ®"""
        terms = []
        soup = BeautifulSoup(html, "lxml")
        content_div = soup.find("div", id="mw-content-text")
        if not content_div:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°æ ¸å¿ƒå†…å®¹åŒºåŸŸ")
            return terms
        
        # å®šä½é”šç‚¹pæ ‡ç­¾ï¼ˆstyle=margin:0;padding:0; + æœ‰idï¼‰
        anchor_ps = content_div.find_all(
            "p",
            attrs={"style": "margin:0;padding:0;", "id": True}
        )
        logger.info(f"ğŸ” æ‰¾åˆ°é”šç‚¹pæ ‡ç­¾æ•°é‡ï¼š{len(anchor_ps)}")
        
        # è§£ææ¯ä¸ªæœ¯è¯­
        for anchor_p in anchor_ps:
            term_name = anchor_p.get("id", "").strip()
            if not term_name:
                continue
            
            # å–ä¸‹ä¸€ä¸ªpæ ‡ç­¾çš„è§£é‡Šï¼ˆç”¨ç»Ÿä¸€æ–‡æœ¬å¤„ç†å‡½æ•°ï¼‰
            next_p = anchor_p.find_next_sibling("p")
            explanation = clean_text(next_p, handle_br=True) if next_p else ""
            # å‰”é™¤è§£é‡Šä¸­é‡å¤çš„æœ¯è¯­å
            if term_name in explanation:
                explanation = explanation.replace(term_name, "").strip()
            
            terms.append({
                "term_name": term_name,
                "term_explanation": explanation,
            })
        
        # å»é‡ï¼ˆç”¨é€šç”¨å»é‡å·¥å…·ï¼‰
        terms = deduplicate_terms(terms)
        logger.info(f"ğŸ“Š è§£æå®Œæˆï¼šæœ‰æ•ˆæœ¯è¯­æ•°é‡ {len(terms)}")
        return terms

    def save(self, terms: list[dict]):
        """ä¿å­˜æœ¯è¯­åˆ°JSONï¼ˆç¡®ä¿ç›®å½•å­˜åœ¨ï¼‰"""
        ensure_output_dir()
        output_path = f"{self.output_dir}/{self.output_filename}"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(terms, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… æœ¯è¯­å·²ä¿å­˜åˆ° {output_path}")

    def run(self) -> list[dict]:
        """ä¸€é”®æ‰§è¡Œï¼šæŠ“å–â†’è§£æâ†’ä¿å­˜ï¼ˆå¯¹å¤–æ ¸å¿ƒæ–¹æ³•ï¼‰"""
        logger.info(f"=== å¼€å§‹é™æ€çˆ¬å–æœ¯è¯­: {self.url} ===")
        try:
            html = self.fetch()
            terms = self.parse(html)
            self.save(terms)
            
            # æ‰“å°å‰5ä¸ªç¤ºä¾‹ï¼ˆè°ƒè¯•ç”¨ï¼‰
            if terms:
                logger.info("\n=== çˆ¬å–ç»“æœç¤ºä¾‹ ===")
                for idx, t in enumerate(terms[:5], 1):
                    logger.info(f"{idx}. åç§°ï¼š{t['term_name']}")
                    logger.info(f"   è§£é‡Šï¼š{t['term_explanation'][:50]}...")
            else:
                logger.warning("âš ï¸  æœªæå–åˆ°æœ‰æ•ˆæœ¯è¯­")
            return terms
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–æµç¨‹å¤±è´¥ï¼š{str(e)}")
            return []

# ä¿ç•™ç‹¬ç«‹æ‰§è¡Œå…¥å£ï¼ˆæ–¹ä¾¿å•ç‹¬è°ƒè¯•ï¼‰
if __name__ == "__main__":
    crawler = TermStaticCrawler()
    crawler.run()