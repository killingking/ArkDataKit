#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åœ¨çº¿æŠ“å– prts.wiki å¹²å‘˜ä¸€è§ˆï¼ˆç¨€æœ‰åº¦+1ä¿®æ­£ï¼‰
è¾“å‡º JSON æ ¼å¼ï¼ˆè§„èŒƒè‹±æ–‡å­—æ®µï¼Œå­—æ®µå«ä¹‰ä»…åœ¨ä»£ç å†…æ³¨é‡Šï¼‰
pip install requests beautifulsoup4
"""

import time
import json
import requests
from bs4 import BeautifulSoup

# é…ç½®é¡¹
URL = 'https://prts.wiki/w/å¹²å‘˜ä¸€è§ˆ'
JSON_FILE = 'operators.json'  # JSONè¾“å‡ºæ–‡ä»¶
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# åŸå§‹HTMLå±æ€§å â†’ è§„èŒƒè‹±æ–‡å­—æ®µåï¼ˆä»£ç å†…æ³¨é‡Šè¯´æ˜å­—æ®µå«ä¹‰ï¼Œä¸è¾“å‡ºåˆ°JSONï¼‰
# --- å­—æ®µå«ä¹‰æ³¨é‡Š ---
# name_cn: å¹²å‘˜ä¸­æ–‡åç§°ï¼ˆå¦‚ï¼šä»¤ã€æµŠå¿ƒæ–¯å¡è’‚ï¼‰
# rarity: ç¨€æœ‰åº¦ï¼ˆPRTSåŸå§‹å€¼0-5ï¼Œå·²+1ä¿®æ­£ä¸º1-6æ˜Ÿï¼Œå¯¹åº”1â˜…-6â˜…ï¼‰
# profession: å¹²å‘˜ä¸»èŒä¸šï¼ˆå¦‚ï¼šå…ˆé”‹ã€åŒ»ç–—ã€è¿‘å«ã€é‡è£…ï¼‰
# sub_profession: èŒä¸šåˆ†æ”¯ï¼ˆå¦‚ï¼šé©­æ¢°æœ¯å¸ˆã€æ·±æµ·æ²»ç–—å¸ˆã€æœ¯æˆ˜è€…ï¼‰
# faction: æ‰€å±é˜µè¥ï¼ˆå¦‚ï¼šç‚ã€æ·±æµ·çŒäººã€ç½—å¾·å²›ã€è±èŒµç”Ÿå‘½ï¼‰
# gender: æ€§åˆ«ï¼ˆå€¼ä»…ä¸ºï¼šç”·/å¥³/æ— ï¼‰
# position: éƒ¨ç½²ä½ç½®ï¼ˆå€¼ä»…ä¸ºï¼šè¿œç¨‹/è¿‘æˆ˜ï¼‰
# tags: å¹²å‘˜æ ‡ç­¾ï¼ˆå¤šä¸ªæ ‡ç­¾ç”¨ä¸­æ–‡é€—å·åˆ†éš”ï¼Œå¦‚ï¼šè´¹ç”¨å›å¤,è¾“å‡º,å¬å”¤ï¼‰
ATTR_MAPPING = {
    'data-zh': 'name_cn',          # å¹²å‘˜ä¸­æ–‡å
    'data-rarity': 'rarity',       # ç¨€æœ‰åº¦ï¼ˆ1-6æ˜Ÿï¼‰
    'data-profession': 'profession',# ä¸»èŒä¸š
    'data-subprofession': 'sub_profession', # èŒä¸šåˆ†æ”¯
    'data-logo': 'faction',        # æ‰€å±é˜µè¥
    'data-sex': 'gender',          # æ€§åˆ«
    'data-position': 'position',   # éƒ¨ç½²ä½ç½®
    'data-tag': 'tags'             # å¹²å‘˜æ ‡ç­¾
}

def fetch_html() -> str:
    """è·å–prtså¹²å‘˜ä¸€è§ˆé¡µé¢çš„HTMLæºç """
    resp = requests.get(URL, headers=HEADERS, timeout=30)
    resp.raise_for_status()  # æ•è·HTTPè¯·æ±‚é”™è¯¯ï¼ˆå¦‚404/500ï¼‰
    resp.encoding = 'utf-8'  # å¼ºåˆ¶UTF-8ç¼–ç ï¼Œé¿å…ä¸­æ–‡ä¹±ç 
    return resp.text

def parse_ops(html: str) -> list[dict]:
    """
    è§£æå¹²å‘˜æ•°æ®
    :param html: é¡µé¢HTMLæºç 
    :return: å¹²å‘˜æ•°æ®åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ ä¸ºè‹±æ–‡å­—æ®µçš„å­—å…¸ï¼‰
    """
    soup = BeautifulSoup(html, 'lxml')
    # å®šä½å¹²å‘˜æ•°æ®æ ¸å¿ƒå®¹å™¨ï¼ˆPRTSé¡µé¢çš„å¹²å‘˜æ•°æ®éƒ½åœ¨è¿™ä¸ªdivé‡Œï¼‰
    data_container = soup.select_one('div#filter-data')
    if not data_container:
        raise RuntimeError('é¡µé¢ç»“æ„å˜æ›´ï¼šæœªæ‰¾åˆ°æ ¸å¿ƒæ•°æ®å®¹å™¨ <div id="filter-data">')
    
    ops_list = []
    # éå†æ¯ä¸ªå¹²å‘˜çš„divèŠ‚ç‚¹ï¼ˆä»…ç›´æ¥å­èŠ‚ç‚¹ï¼Œé¿å…é€’å½’ï¼‰
    for op_div in data_container.find_all('div', recursive=False):
        # æå–åŸå§‹å±æ€§å€¼
        raw_data = {attr: op_div.get(attr, '').strip() for attr in ATTR_MAPPING.keys()}
        
        # å…³é”®ä¿®æ­£ï¼šç¨€æœ‰åº¦+1ï¼ˆPRTSåŸå§‹æ•°æ®æ˜¯0-5ï¼Œå¯¹åº”1-6æ˜Ÿï¼‰
        raw_rarity = raw_data['data-rarity'] or '0'
        raw_data['data-rarity'] = str(int(raw_rarity) + 1)
        
        # æ˜ å°„ä¸ºè§„èŒƒçš„è‹±æ–‡å­—æ®µ
        op_data = {ATTR_MAPPING[old_key]: value for old_key, value in raw_data.items()}
        ops_list.append(op_data)
    
    return ops_list

def save_json(ops_list: list[dict]):
    """
    å°†å¹²å‘˜æ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ˆç®€æ´ç»“æ„ï¼Œä»…å«å…ƒä¿¡æ¯+å¹²å‘˜åˆ—è¡¨ï¼‰
    :param ops_list: è§£æåçš„å¹²å‘˜æ•°æ®åˆ—è¡¨
    """
    # æ’åºè§„åˆ™ï¼šç¨€æœ‰åº¦é™åºï¼ˆ6æ˜Ÿåœ¨å‰ï¼‰â†’ ä¸­æ–‡åå‡åºï¼ˆæ‹¼éŸ³æ’åºï¼‰
    ops_list.sort(key=lambda x: (-int(x['rarity']), x['name_cn']))
    
    # æ„é€ æœ€ç»ˆçš„JSONæ•°æ®ç»“æ„ï¼ˆä»…ä¿ç•™å…ƒä¿¡æ¯+å¹²å‘˜åˆ—è¡¨ï¼Œæ— é¢å¤–æ³¨é‡ŠèŠ‚ç‚¹ï¼‰
    final_json = {
        "meta_info": {  # å…ƒä¿¡æ¯ï¼ˆåŸºç¡€ç»Ÿè®¡ï¼‰
            "update_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
            "total_operators": len(ops_list),
            "data_source": URL
        },
        "operators": ops_list  # å¹²å‘˜æ•°æ®åˆ—è¡¨ï¼ˆæ ¸å¿ƒæ•°æ®ï¼‰
    }
    
    # å†™å…¥JSONæ–‡ä»¶ï¼ˆensure_ascii=Falseä¿ç•™ä¸­æ–‡ï¼Œindent=2æ ¼å¼åŒ–ï¼‰
    with open(JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_json, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    print(f"âœ… æ•°æ®ä¿å­˜å®Œæˆï¼æ–‡ä»¶è·¯å¾„ï¼š{JSON_FILE}")
    print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡ï¼šå…± {len(ops_list)} åå¹²å‘˜")

def main():
    """ä¸»æ‰§è¡Œæµç¨‹ï¼ˆå¼‚å¸¸æ•è·+å‹å¥½æç¤ºï¼‰"""
    try:
        print("ğŸ” å¼€å§‹æŠ“å–PRTSå¹²å‘˜æ•°æ®...")
        html = fetch_html()
        ops_data = parse_ops(html)
        save_json(ops_data)
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼š{str(e)}ï¼ˆæ£€æŸ¥ç½‘ç»œæˆ–URLæ˜¯å¦æœ‰æ•ˆï¼‰")
    except RuntimeError as e:
        print(f"âŒ æ•°æ®è§£æå¤±è´¥ï¼š{str(e)}ï¼ˆå¯èƒ½æ˜¯PRTSé¡µé¢ç»“æ„å˜æ›´ï¼‰")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼š{type(e).__name__} - {str(e)}")

if __name__ == '__main__':
    main()