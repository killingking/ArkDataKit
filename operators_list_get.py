# operators_list_get.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import time
import json
import requests
from bs4 import BeautifulSoup
from config import HEADERS, OPERATOR_LIST_CONFIG, JSON_OUTPUT_DIR
from utils import logger, ensure_output_dir

class OperatorListCrawler:
    """å¹²å‘˜ä¸€è§ˆçˆ¬å–å™¨ï¼ˆè½»é‡ç±»å°è£…ï¼Œæ— çŠ¶æ€ï¼‰"""
    def __init__(self):
        self.url = OPERATOR_LIST_CONFIG["url"]
        self.headers = HEADERS
        self.attr_mapping = OPERATOR_LIST_CONFIG["attr_mapping"]
        self.output_dir = JSON_OUTPUT_DIR
        self.output_filename = OPERATOR_LIST_CONFIG["json_output"]

    def fetch(self) -> str:
        """æŠ“å–å¹²å‘˜ä¸€è§ˆé¡µé¢HTML"""
        try:
            resp = requests.get(self.url, headers=self.headers, timeout=30)
            resp.raise_for_status()
            resp.encoding = 'utf-8'
            logger.info(f"âœ… æˆåŠŸè·å–å¹²å‘˜ä¸€è§ˆé¡µé¢ï¼š{self.url}")
            return resp.text
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼š{str(e)}ï¼ˆæ£€æŸ¥ç½‘ç»œæˆ–URLæ˜¯å¦æœ‰æ•ˆï¼‰")
            raise

    def parse(self, html: str) -> list[dict]:
        """è§£æå¹²å‘˜ä¸€è§ˆæ•°æ®"""
        soup = BeautifulSoup(html, 'lxml')
        data_container = soup.select_one('div#filter-data')
        if not data_container:
            raise RuntimeError('âŒ é¡µé¢ç»“æ„å˜æ›´ï¼šæœªæ‰¾åˆ°æ ¸å¿ƒæ•°æ®å®¹å™¨ <div id="filter-data">')
        
        ops_list = []
        # éå†æ¯ä¸ªå¹²å‘˜çš„divèŠ‚ç‚¹ï¼ˆä»…ç›´æ¥å­èŠ‚ç‚¹ï¼Œé¿å…é€’å½’ï¼‰
        for op_div in data_container.find_all('div', recursive=False):
            # æå–åŸå§‹å±æ€§å€¼
            raw_data = {attr: op_div.get(attr, '').strip() for attr in self.attr_mapping.keys()}
            
            # å…³é”®ä¿®æ­£ï¼šç¨€æœ‰åº¦+1ï¼ˆPRTSåŸå§‹æ•°æ®æ˜¯0-5ï¼Œå¯¹åº”1-6æ˜Ÿï¼‰
            raw_rarity = raw_data['data-rarity'] or '0'
            raw_data['data-rarity'] = str(int(raw_rarity) + 1)
            
            # æ˜ å°„ä¸ºè§„èŒƒçš„è‹±æ–‡å­—æ®µ
            op_data = {self.attr_mapping[old_key]: value for old_key, value in raw_data.items()}
            ops_list.append(op_data)
        
        logger.info(f"ğŸ“Š è§£æå®Œæˆï¼šå…±æå– {len(ops_list)} åå¹²å‘˜åŸºç¡€ä¿¡æ¯")
        return ops_list

    def save(self, ops_list: list[dict]):
        """ä¿å­˜å¹²å‘˜ä¸€è§ˆæ•°æ®åˆ°JSON"""
        # æ’åºè§„åˆ™ï¼šç¨€æœ‰åº¦é™åºï¼ˆ6æ˜Ÿåœ¨å‰ï¼‰â†’ ä¸­æ–‡åå‡åºï¼ˆæ‹¼éŸ³æ’åºï¼‰
        ops_list.sort(key=lambda x: (-int(x['rarity']), x['name_cn']))
        
        # æ„é€ æœ€ç»ˆçš„JSONæ•°æ®ç»“æ„
        final_json = {
            "meta_info": {  # å…ƒä¿¡æ¯ï¼ˆåŸºç¡€ç»Ÿè®¡ï¼‰
                "update_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
                "total_operators": len(ops_list),
                "data_source": self.url
            },
            "operators": ops_list  # å¹²å‘˜æ•°æ®åˆ—è¡¨ï¼ˆæ ¸å¿ƒæ•°æ®ï¼‰
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        ensure_output_dir()
        output_path = f"{self.output_dir}/{self.output_filename}"
        
        # å†™å…¥JSONæ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… å¹²å‘˜ä¸€è§ˆæ•°æ®ä¿å­˜å®Œæˆï¼æ–‡ä»¶è·¯å¾„ï¼š{output_path}")
        logger.info(f"ğŸ“Š æŠ“å–ç»Ÿè®¡ï¼šå…± {len(ops_list)} åå¹²å‘˜")

    def run(self) -> list[dict]:
        """ä¸€é”®æ‰§è¡Œï¼šæŠ“å–â†’è§£æâ†’ä¿å­˜"""
        logger.info("=== å¼€å§‹æŠ“å–PRTSå¹²å‘˜ä¸€è§ˆæ•°æ® ===")
        try:
            html = self.fetch()
            ops_data = self.parse(html)
            self.save(ops_data)
            return ops_data
        except RuntimeError as e:
            logger.error(f"âŒ æ•°æ®è§£æå¤±è´¥ï¼š{str(e)}ï¼ˆå¯èƒ½æ˜¯PRTSé¡µé¢ç»“æ„å˜æ›´ï¼‰")
            raise
        except Exception as e:
            logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼š{type(e).__name__} - {str(e)}")
            raise

# ä¿ç•™ç‹¬ç«‹æ‰§è¡Œå…¥å£ï¼ˆæ–¹ä¾¿å•ç‹¬è°ƒè¯•ï¼‰
if __name__ == '__main__':
    crawler = OperatorListCrawler()
    crawler.run()